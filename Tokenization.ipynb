{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apoak/Deep-Learning-Projects/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFsOugnVCS-8"
      },
      "source": [
        "### Lab 8.1 Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NAp22wcCS-_"
      },
      "source": [
        "This week we will work up to creating an RNN text generator.  In today's lab you will explore different methods of text tokenization.   Here's an overview of what you will try to do.\n",
        "\n",
        "Imagine that our entire dataset consists of the following text:\n",
        "\n",
        "    hello world hello a b c\n",
        "\n",
        "We would first build a vocabulary of the words in the dataset:\n",
        "\n",
        "    0: hello\n",
        "    1: world\n",
        "    2: a\n",
        "    3: b\n",
        "    4: c\n",
        "\n",
        "Thus the dataset can be mapped to token indices:\n",
        "\n",
        "    0 1 0 2 3 4\n",
        "\n",
        "Now suppose that we have defined the maximum sequence length (`seq_len`) to be 3.  We will use each possible sequence as the input to our RNN, and the next token as the target.  Here are the possible input sequences and targets:\n",
        "\n",
        "    0 1 0 -> 2\n",
        "    1 0 2 -> 3\n",
        "    0 2 3 -> 4\n",
        "\n",
        "You will build a subclass of `Dataset` to find all possible sequences for a given dataset, either at the word or character level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjSxtPSCCS_A"
      },
      "source": [
        "The following code will download the text of Shakespeare's sonnets and read it in as one long string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLylStb8CS_A"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIVi1ND5CS_B"
      },
      "outputs": [],
      "source": [
        "!wget --no-clobber \"https://www.dropbox.com/scl/fi/7r68l64ijemidyb9lf80q/sonnets.txt?rlkey=udb47coatr2zbrk31hsfbr22y&dl=1\" -O sonnets.txt\n",
        "text = (open(\"sonnets.txt\").read())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydIa7oUpCS_C"
      },
      "outputs": [],
      "source": [
        "text = text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU_AfvmDCS_C"
      },
      "outputs": [],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwnyBkzACS_D"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1. Prepare a vocabulary of the unique words in the dataset.  (For simplicity's sake you can leave the punctuation in.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text.split()\n",
        "split_text = text.split()\n",
        "vocab = {}\n",
        "for word in split_text:\n",
        "  if word not in vocab:\n",
        "    vocab[word] = word"
      ],
      "metadata": {
        "id": "etuZwHgRDZa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKZk5rP7CS_D"
      },
      "source": [
        "2. Now you will make a Dataset subclass that can return sequences of tokens, encoded as integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyEKXO_9CS_E"
      },
      "outputs": [],
      "source": [
        "class WordDataset(Dataset):\n",
        "  def __init__(self,text,seq_len=100):\n",
        "    self.split_text = text.lstrip(\"\\ufeff\").split()\n",
        "    self.seq_len = seq_len\n",
        "    # add code to compute the vocabulary (copied from exercise 1)\n",
        "    self.vocab = {}\n",
        "    # add code to convert the text to a sequence of word indices\n",
        "    self.word_indices = {}\n",
        "    self.token_sequence = []\n",
        "\n",
        "    self.indices_word = {}\n",
        "\n",
        "\n",
        "  def init_vocab(self):\n",
        "    self.vocab.update({char: char for char in self.split_text if char not in self.vocab})\n",
        "\n",
        "  def init_token_sequence(self):\n",
        "    self.token_sequence = [self.word_indices[self.split_text[idx]] for idx in range(len(self.split_text))]\n",
        "\n",
        "  def indices(self):\n",
        "    self.word_indices = {word: i for i, word in enumerate(self.vocab) if word not in self.word_indices}\n",
        "    self.indices_word = {i: word for word, i in self.word_indices.items()}\n",
        "\n",
        "  def __len__(self):\n",
        "    # replace this with code to return the number of possible sub-sequences\n",
        "    return len(self.token_sequence) - self.seq_len\n",
        "\n",
        "  # A little confused what this method really does!\n",
        "  def __getitem__(self,i):\n",
        "    # replace this with code to return a sequence of length seq_len of token indices starting at i, and the index of token i+seq_len as the label\n",
        "    sequence = [self.token_sequence[idx] for idx in range(i, i+self.seq_len)]\n",
        "    label = self.token_sequence[i+self.seq_len]\n",
        "    pair = (label, sequence)\n",
        "    return pair\n",
        "\n",
        "  def decode(self,tokens):\n",
        "    # replace this with code to convert a sequence of tokens back into a string\n",
        "    string = \"\"\n",
        "    for token in tokens:\n",
        "      string = string + self.indices_word[token] + \" \"\n",
        "    return string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNhkl-dcCS_E"
      },
      "source": [
        "3. Verify that your class can successfully encode and decode sequences."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = WordDataset(text)\n",
        "ds.init_vocab()\n",
        "ds.indices()\n",
        "ds.init_token_sequence()\n",
        "\n",
        "tokens = ds.__getitem__(29)\n",
        "\n",
        "sequence = ds.decode(tokens[1])\n",
        "print(\"Vocabulary: \", ds.vocab)\n",
        "print(\"Word to Indice hash: \",  ds.word_indices)\n",
        "print(\"Indices to Word hash: \",  ds.indices_word)\n",
        "print(\"Possible subsequences: \", ds.__len__())\n",
        "print(\"Encoded tokens: \", ds.__getitem__(29))\n",
        "print(\"Decoded tokens: \", sequence)\n",
        "\n"
      ],
      "metadata": {
        "id": "tG8GtOYvFxiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R34hQm89CS_E"
      },
      "source": [
        "4. Do the exercise again, but this time at the character level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsQMezgWCS_E"
      },
      "outputs": [],
      "source": [
        "class CharacterDataset(Dataset):\n",
        "  def __init__(self,text,seq_len=100):\n",
        "    self.chunk_text = text.lstrip(\"\\ufeff\").replace(\"\\n\",\"\")\n",
        "    self.seq_len = seq_len\n",
        "    # add code to convert the text to a sequence of word indices\n",
        "    self.vocab = {}\n",
        "    # add code to compute the vocabulary of unique characters\n",
        "    self.char_tokens = {}\n",
        "    self.tokens_char = {}\n",
        "    self.token_sequence = []\n",
        "\n",
        "    # add code to convert the text to a sequence of character indices\n",
        "  def init_vocab(self):\n",
        "    self.vocab.update({char: char for char in self.chunk_text if char not in self.vocab})\n",
        "\n",
        "  def init_token_sequence(self):\n",
        "    self.token_sequence = [self.char_tokens[self.chunk_text[idx]] for idx in range(len(self.chunk_text))]\n",
        "\n",
        "  def indices(self):\n",
        "    self.char_tokens = {char: i for i, char in enumerate(self.vocab) if char not in self.char_tokens}\n",
        "    self.tokens_char = {i: char for char, i in self.char_tokens.items()}\n",
        "\n",
        "  def __len__(self):\n",
        "    # replace this with code to return the number of possible sub-sequences\n",
        "    return len(self.token_sequence) - self.seq_len\n",
        "\n",
        "# A little confused what this method really does!\n",
        "  def __getitem__(self,i):\n",
        "  # replace this with code to return the sequence of length seq_len of token indices starting at i, and the index of token i+seq_len as the label\n",
        "    sequence = [self.token_sequence[idx] for idx in range(i, i+self.seq_len)]\n",
        "    label = self.char_tokens[self.chunk_text[i+self.seq_len]]\n",
        "    pair = (label, sequence)\n",
        "    return pair\n",
        "\n",
        "  def decode(self,tokens):\n",
        "    # replace this with code to convert a sequence of tokens back into a string\n",
        "    sequence = \"\".join([self.tokens_char[token]for token in tokens])\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = CharacterDataset(text)\n",
        "ds.init_vocab()\n",
        "ds.indices()\n",
        "ds.init_token_sequence()\n",
        "\n",
        "tokens = ds.__getitem__(100)\n",
        "\n",
        "sequence = ds.decode(tokens[1])\n",
        "print(ds.token_sequence)\n",
        "print(\"Vocabulary: \", ds.vocab)\n",
        "print(\"Word to Indice hash: \",  ds.char_tokens)\n",
        "print(\"Indices to Word hash: \",  ds.tokens_char)\n",
        "print(\"Possible subsequences: \", ds.__len__())\n",
        "print(\"Encoded tokens: \", ds.__getitem__(100))\n",
        "print(\"Decoded tokens: \", sequence)"
      ],
      "metadata": {
        "id": "LeOtQxNmRbcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y1MojSKCS_E"
      },
      "source": [
        "5. Compare the number of sequences for each tokenization method."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of sequences for the word tokenization is 17,570 while the number for the character tokenization is 95,202. This difference makes sense because the amount of data in each window of sequence length is very different. One is able to process 100 words at a time while the other is only able to process 100 letters at a time."
      ],
      "metadata": {
        "id": "TjBIMy25PE2W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKOeUzSjCS_F"
      },
      "source": [
        "6. Optional: implement the byte pair encoding algorithm to make a Dataset class that uses word parts."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}