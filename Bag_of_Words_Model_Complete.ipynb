{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apoak/Deep-Learning-Projects/blob/main/Bag_of_Words_Model_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lab 7.1: Bag of Words Model\n",
        "\n",
        "In this lab you will use the bag of words model to learn author attribution with a [dataset of texts from Victorian authors](https://github.com/agungor2/Authorship_Attribution?tab=readme-ov-file)."
      ],
      "metadata": {
        "id": "M4X00o6AYKEl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNzZsS19pFxI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we download the CSV file containing the text snippets and author IDs."
      ],
      "metadata": {
        "id": "R6EgVxxbcGBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-clobber -O Gungor_2018_VictorianAuthorAttribution_data-train.csv -q https://www.dropbox.com/scl/fi/emk9db05t9u8yzgrjje7t/Gungor_2018_VictorianAuthorAttribution_data-train.csv?rlkey=kzvbl0mbpnrpjr4c3q18le6w2&dl=1"
      ],
      "metadata": {
        "id": "nHGLy49dpcvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Gungor_2018_VictorianAuthorAttribution_data-train.csv', encoding = \"ISO-8859-1\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "NEczcgxSplh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = list(df['text'])\n",
        "labels = df['author'].values"
      ],
      "metadata": {
        "id": "PGpnyq9EpvE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[0]"
      ],
      "metadata": {
        "id": "BPDH_z93QOxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises\n",
        "\n",
        "1. Use the `CountVectorizer` from `sklearn.feature_extraction.text` to produce a term frequency vector for each text.  Set `max_features=1000` to only use the top 1000 terms.\n",
        "\n",
        "Prepare a 90/10 train-test split `random_state=42`.\n",
        "\n",
        "Train the default `MLPCLassifier` from `sklearn.neural_network` on the data and report the train and test accuracy.  You can use the argument `verbose=True` to `MLPClassifier` to monitor training."
      ],
      "metadata": {
        "id": "qMm2D-6CcMpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "iHE2nJXCXEG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(text)\n",
        "# print(vectorizer.get_feature_names_out())\n",
        "# print(X.toarray())\n",
        "\n"
      ],
      "metadata": {
        "id": "bPKXRSRctLGr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "oq0sbwUqYleq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLPClassifier(verbose=True)\n",
        "mlp.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "L8_ctcAs9DMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "8wx-7pjB9NZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. Repeat the steps but using `TfidfVectorizer` to produce term frequency - inverse document frequency vectors.\n",
        "\n",
        "Does the IDF weighting improve the results?"
      ],
      "metadata": {
        "id": "bscBHmXududo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "Hu-ut-ke-4p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfid = TfidfVectorizer(max_features=1000)\n",
        "X2 = vectorizer.fit_transform(text)"
      ],
      "metadata": {
        "id": "oVHb29MztMlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, labels, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "8W45RKHJ_EEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp2 = MLPClassifier(verbose=True)\n",
        "mlp2.fit(X_train2, y_train2)"
      ],
      "metadata": {
        "id": "VyOaCVjk_GwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "y_pred2 = mlp2.predict(X_test2)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy2 = accuracy_score(y_test2, y_pred2)\n",
        "print(f\"Test Accuracy: {accuracy2:.4f}\")"
      ],
      "metadata": {
        "id": "rKiJyfAZ_Pq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison:**\n",
        "The accuracy of the two methods of Vectorization is equal, the most notable difference is the TfidfVectorizer took more epochs to minimize the loss."
      ],
      "metadata": {
        "id": "-S4V9Vb7_8kp"
      }
    }
  ]
}