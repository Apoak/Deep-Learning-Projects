{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apoak/Deep-Learning-Projects/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od1MCsjuzyla",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers transformers torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNzZsS19pFxI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLoLtUCUZ6J6"
      },
      "outputs": [],
      "source": [
        "# device = 'cpu'\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHGLy49dpcvv"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('IMDB-Dataset.csv'):\n",
        "  !wget -O IMDB-Dataset.csv -q \"https://www.dropbox.com/scl/fi/0c7zc2adk1mgwgut5w80w/IMDB-Dataset.csv?rlkey=1drfg4zw36mhu32ndy2ihnygw&dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEczcgxSplh6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('IMDB-Dataset.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGpnyq9EpvE4"
      },
      "outputs": [],
      "source": [
        "text = list(df['review'].str.replace('<br />',''))\n",
        "labels = np.array(df['sentiment'].map({'negative':0,'positive':1}))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NeUyMIdovaY",
        "outputId": "66bc6a19-5481-41e4-b2cc-1ceb818cc3f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 2)\n",
            "(50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63sVLwZ1xhFB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer #, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "# model = AutoModel.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xGZ7zEgaNZv"
      },
      "source": [
        "Example of how to tokenize text:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO9817pyaQrn"
      },
      "outputs": [],
      "source": [
        "# seq = text[0][:10]\n",
        "seq = text[0][:10]\n",
        "seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Kr2qH8HaPll"
      },
      "outputs": [],
      "source": [
        "seq = seq + \"[0]\"\n",
        "print(seq)\n",
        "tokens = tokenizer.tokenize(seq)\n",
        "print(tokens)\n",
        "token_ids = tokenizer(seq)['input_ids']\n",
        "token_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR6rV-W3ZBsO"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(token_ids+[0,0,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of words model.**\n",
        "\n",
        " Create a 90/10 train/test split of the data. Create TF-IDF\n",
        "weighted histograms (using TfidfVectorizer) using the top 1000 words and train\n",
        "an MLP model (MLPClassifier) to classify them. Compute the train and test\n",
        "accuracy of the model (using the .score() function)."
      ],
      "metadata": {
        "id": "99TjKRROETKv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StP-erbAayCO"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(text, labels, test_size=0.1)\n",
        "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_features=1000)\n",
        "x_train_V = vectorizer.fit_transform(x_train)\n",
        "x_test_V = vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = sklearn.neural_network.MLPClassifier()\n",
        "mlp.fit(x_train_V, y_train)"
      ],
      "metadata": {
        "id": "Iqs5Neh8Esdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test: \", mlp.score(x_test_V, y_test))\n",
        "print(\"Train: \", mlp.score(x_train_V, y_train))"
      ],
      "metadata": {
        "id": "34dlbVsKE3pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN model:**\n",
        "\n",
        "Train a GRU to process sequences of BPE tokens output a binary\n",
        "sentiment prediction. (Donâ€™t forget to set the batch_first flag if needed!)\n",
        "Use an Embedding layer to map the BPE tokens to embedding vectors for input\n",
        "to the GRU.\n",
        "If the text is too long, take a random sub-sequence; if the text is too short, pad it\n",
        "using token index 0."
      ],
      "metadata": {
        "id": "bDF6sF1EGXRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split raw data first\n",
        "train_texts, test_texts, train_labels, test_labels = sklearn.model_selection.train_test_split(text, labels, test_size=0.1)\n",
        "# validation_texts, t_texts, validation_labels, t_labels = sklearn.model_selection.train_test_split(train_texts, train_labels, test_size=0.5)\n",
        "max_seq_length = 100\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length, embedding):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.embedding = embedding\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    # The dataloader is smart, calls getitem under the hood and gives it the current idx to fetch the cooresponding review.\n",
        "    def __getitem__(self, idx):\n",
        "        # Process each review individually\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        # Truncate if needed\n",
        "        if len(text) > self.max_length:\n",
        "            start_idx = random.randint(0, len(text) - self.max_length)\n",
        "            text = text[start_idx:start_idx + self.max_length]\n",
        "\n",
        "        # Tokenize and add padding\n",
        "        tokens = self.tokenizer(text, padding='max_length', truncation=True,\n",
        "                                max_length=self.max_length, return_tensors=\"pt\")\n",
        "\n",
        "        # Get embeddings\n",
        "        with torch.no_grad():  # Important: don't build computation graph during dataset creation\n",
        "            embedded = self.embedding(tokens['input_ids'].squeeze(0))\n",
        "\n",
        "        return embedded, self.labels[idx]\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "embedding_layer = nn.Embedding(num_embeddings=len(tokenizer), embedding_dim=100)\n",
        "\n",
        "train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, max_seq_length, embedding_layer)\n",
        "test_dataset = ReviewDataset(test_texts, test_labels, tokenizer, max_seq_length, embedding_layer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# validation_dataset = ReviewDataset(validation_texts, validation_labels, tokenizer, max_seq_length, embedding_layer)\n",
        "# validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "Tv0blhJaBzob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenized sequence:**"
      ],
      "metadata": {
        "id": "j7lVkjcZNHdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random"
      ],
      "metadata": {
        "id": "czxSQY7d2qmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU INITIALIZATION:**"
      ],
      "metadata": {
        "id": "MpzK1EykqtBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate):\n",
        "    super(GRU, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    self.input_projection = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "    # self.sigmoid = nn.Sigmoid()\n",
        "    dropout=dropout_rate\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        out, hn = self.gru(x, h0)  # out is the GRU outputs, hn is the final hidden state\n",
        "\n",
        "        # Resnet idea\n",
        "        if x.size(2) != self.hidden_size:  # check if input size and hidden size are the same\n",
        "          x = self.input_projection(x)\n",
        "\n",
        "        out = out + x\n",
        "\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "\n",
        "        #out = self.fc(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        # out = self.sigmoid(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hRfqhTUZF7Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperperameter tuning:**"
      ],
      "metadata": {
        "id": "Y0q9yJSAsRIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gru = GRU(\n",
        "    input_size = 100,\n",
        "    hidden_size = 128,\n",
        "    num_layers= 3,\n",
        "    output_size=1,\n",
        "    dropout_rate=0.3)\n",
        "\n",
        "gru = gru.to(device)"
      ],
      "metadata": {
        "id": "RN7xfJdwrFKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 3e-4\n",
        "# lr = 1e-3\n",
        "\n",
        "opt = torch.optim.Adam(gru.parameters(),lr=lr)\n",
        "# opt = torch.optim.AdamW(gru.parameters(),lr=lr, weight_decay=1e-4)\n",
        "\n",
        "#pos_weight = torch.tensor([2.0]).to(device)  # Give 2x importance to positive class\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "# loss_fn = nn.BCELoss()\n",
        "\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "pUTiAfJFarxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU Training:**"
      ],
      "metadata": {
        "id": "Yernf1CFp9dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train gru\n",
        "def train_gru(gru, train_loader, epochs, optimizer, criterion):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    gru.train()\n",
        "    for x_batch, y_batch in train_loader:\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      y_pred = gru(x_batch)\n",
        "      y_pred = y_pred.view(-1)\n",
        "      #y_pred = y_pred.squeeze()\n",
        "      # print(y_batch.shape)\n",
        "      # print(y_pred.shape)\n",
        "\n",
        "      #loss = loss_fn(output.view(-1,len(ds.vocabulary)),y_batch.view(-1))\n",
        "\n",
        "      loss = criterion(y_pred,y_batch.float())\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(gru.parameters(), max_norm=1)\n",
        "      optimizer.step()\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "O6NL5Mq8L1Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gru(gru, train_loader, epochs, opt, loss_fn)"
      ],
      "metadata": {
        "id": "buf_nwv6tpX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU Evaluation:**"
      ],
      "metadata": {
        "id": "j7-BSYZap0yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_gru(gru, test_loader):\n",
        "  gru.eval()\n",
        "  metric = torchmetrics.classification.BinaryAccuracy().to(device) #(task='multiclass', num_classes=len(tokenizer)).to(device)\n",
        "  with torch.no_grad():\n",
        "    for x_batch, y_batch in test_loader:\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "      y_pred = gru(x_batch)\n",
        "      # y_pred = y_pred.squeeze()\n",
        "      y_pred = torch.sigmoid(y_pred).squeeze()\n",
        "      # metric(y_pred.to('cpu'), y_batch.to('cpu'))\n",
        "      # acc = metric.compute().item()\n",
        "      # print(f\"Accuracy: {acc}\")\n",
        "      # batch_acc = torchmetrics.functional.accuracy(y_pred, y_batch, task=\"binary\").item()\n",
        "      # print(f\"Accuracy = {batch_acc:.4f}\")\n",
        "      metric.update(y_pred, y_batch)  # Accumulate accuracy across batches\n",
        "\n",
        "    acc = metric.compute().item()  # Compute accuracy after all batches\n",
        "    print(f\"Final Accuracy: {acc}\")"
      ],
      "metadata": {
        "id": "UqbClGQOp6Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gru(gru, test_loader)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_gJ6UArV_z5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VALIDATION TEST:**"
      ],
      "metadata": {
        "id": "zcaMq3gN5PY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_gru(gru, train_loader)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aFdbwHZc5MeP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}