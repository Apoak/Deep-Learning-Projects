{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apoak/Deep-Learning-Projects/blob/main/Attention_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8_JRrHbuqxD"
      },
      "source": [
        "### Lab 9.1 Attention Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wca8laewuqxF"
      },
      "source": [
        "This week you will experimenet with attention-based models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQeu1KuyuqxF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9kaAydLuqxG"
      },
      "source": [
        "1. Complete the following implementation of scaled dot-product attention.   Run the code cell to verify that the output shape is what it should be.\n",
        "\n",
        "*Note: you can use `scores = scores.masked_fill(...)` to fill in values where the mask is True.  Fill in -1e9 as the score for masked values.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RxPBjrsuqxG"
      },
      "outputs": [],
      "source": [
        "def attention(Q,K,V,mask=None):\n",
        "  \"\"\"\n",
        "  Computes scaled dot-product attention.\n",
        "\n",
        "  Compute scores as Q*K^T.\n",
        "  Optionally mask out score values to -1e9 where the mask is True.\n",
        "  Divide by sqrt(d_k).\n",
        "  Compute softmax on scores along the rows to obtain attention weights.\n",
        "  Matrix multiply attention weights by values.\n",
        "\n",
        "  Arguments:\n",
        "    Q: queries [B,L,d_k]\n",
        "    K: keys    [B,S,d_k]\n",
        "    V: values  [B,S,d_v]\n",
        "    mask: optional Boolean mask where True means hidden [B,L,S]\n",
        "\n",
        "  Returns:\n",
        "    Sequence of context vectors of shape [B,L,d_v]\n",
        "  \"\"\"\n",
        "  # attention(Q, K, V) = softmax(QK^T/(d_k^.5))V\n",
        "  scores = Q @ K.transpose(-2,-1)\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask, -1e9)\n",
        "  scores = scores/np.sqrt(Q.shape[-1])\n",
        "  scores = F.softmax(scores,dim=-1)\n",
        "  scores = scores @ V\n",
        "\n",
        "  return scores\n",
        "\n",
        "Q = torch.rand(1,5,64)\n",
        "K = torch.rand(1,10,64)\n",
        "V = torch.rand(1,10,8)\n",
        "mask = (torch.rand(1,5,10)>0.5)\n",
        "\n",
        "y = attention(Q,K,V,mask=mask)\n",
        "\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sKJXN8mY0rc"
      },
      "source": [
        "The following code creates classes to build a Transformer-style decoder for generating sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4YQt8IeuqxH"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self,d_model,d_k):\n",
        "        super().__init__()\n",
        "        self.WQ = nn.Linear(d_model,d_k)\n",
        "        self.WK = nn.Linear(d_model,d_k)\n",
        "        self.WV = nn.Linear(d_model,d_k)\n",
        "\n",
        "    def forward(self,Q,K,V,mask=None):\n",
        "        \"\"\" Compute attention head.\n",
        "\n",
        "            Project the input to queries, keys, and values, and then apply attention.\n",
        "            Arguments:\n",
        "                Q: queries [B,L,d_model]\n",
        "                K: keys    [B,S,d_model]\n",
        "                V: values  [B,L,d_model]\n",
        "                mask: optional Boolean mask where True means hidden [B,L,S]\n",
        "            Output:\n",
        "                Context vectors [B,L,d_k]\n",
        "        \"\"\"\n",
        "        # apply linear projections to queries, keys, and values followed by masked attention\n",
        "        return attention(self.WQ(Q),self.WK(K),self.WV(V),mask=mask)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,d_model=512,num_heads=8):\n",
        "        super().__init__()\n",
        "        self.heads = []\n",
        "        d_k = d_model // num_heads\n",
        "        self.heads = nn.ModuleList([AttentionHead(d_model,d_k) for head in range(num_heads)])\n",
        "        self.W = nn.Linear(d_model,d_model)\n",
        "\n",
        "    def forward(self,Q,K,V,mask=None):\n",
        "        \"\"\" Compute multi-head attention.\n",
        "\n",
        "            Applies attention num_heads times, concatenates the results, and applies a final linear projection.\n",
        "            Arguments:\n",
        "                Q: queries [B,L,d_model]\n",
        "                K: keys    [B,S,d_model]\n",
        "                V: values  [B,L,d_model]\n",
        "                mask: optional Boolean mask where True means hidden [B,L,S]\n",
        "            Output:\n",
        "               result of multi-head attention [B,L,d_model]\n",
        "        \"\"\"\n",
        "        # compute each attention head and concatenate\n",
        "        h = torch.cat([head(Q,K,V,mask=mask) for head in self.heads],dim=-1)\n",
        "\n",
        "        # apply output projection\n",
        "        return self.W(h)\n",
        "\n",
        "class SelfAttentionBlock(nn.Module):\n",
        "    def __init__(self,d_model=512,num_heads=8,d_ff=2048):\n",
        "        super().__init__()\n",
        "        self.multi_head_attention = MultiHeadAttention(d_model,num_heads)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model,d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff,d_model),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self,x,mask=None):\n",
        "        \"\"\" Compute self attention block.\n",
        "\n",
        "            Arguments:\n",
        "                x: input sequence [B,S,d_model]\n",
        "                mask: optional Boolean mask where True means hidden [B,L,S]\n",
        "            Output:\n",
        "               result of attention block [B,L,d_model]\n",
        "        \"\"\"\n",
        "        # compute multi-head attention\n",
        "        mha = self.multi_head_attention(x,x,x,mask=mask)\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        x = self.ln1(mha + x)\n",
        "\n",
        "        # compute feed-forward network\n",
        "        ff = self.feed_forward(x)\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        x = self.ln2(ff + x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self,max_seq_len,d_model):\n",
        "        super().__init__()\n",
        "        self.positional_embedding = nn.Embedding(max_seq_len,d_model)\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\" Adds a positional embedding.\n",
        "\n",
        "            Arguments:\n",
        "                x: input token sequence [B,S,d_model]\n",
        "            Output:\n",
        "               sequence with positional embedding added [B,S,d_model]\n",
        "        \"\"\"\n",
        "        # get sequence length\n",
        "        N = x.shape[1]\n",
        "\n",
        "        # look up positional embedding vectors\n",
        "        pe = self.positional_embedding(torch.arange(N).to(x.device)) # [N,d_model]\n",
        "\n",
        "        # add to input\n",
        "        x = x + pe[None,...] # [B,N,d_model]\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self,vocabulary_size,max_seq_len,\n",
        "                      d_model=512,num_heads=8,d_ff=2048,num_blocks=6):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([SelfAttentionBlock(d_model,num_heads,d_ff) for b in range(num_blocks)])\n",
        "        self.token_embedding = nn.Embedding(vocabulary_size,d_model)\n",
        "        self.output = nn.Linear(d_model,vocabulary_size)\n",
        "        self.positional_embedding = PositionalEmbedding(max_seq_len,d_model)\n",
        "\n",
        "    def forward(self,x,mask=None):\n",
        "        \"\"\" Computes the decoded sequence:\n",
        "\n",
        "            Convert input to token embedding vectors\n",
        "            Add positional embedding to input\n",
        "            Apply self-attention blocks with mask\n",
        "            Compute output\n",
        "\n",
        "            Arguments:\n",
        "                x: input token sequence [B,S]\n",
        "                mask: optional Boolean mask where false means hidden [B,S]\n",
        "            Output:\n",
        "               sequence predictions [B,S,output_size]\n",
        "        \"\"\"\n",
        "        # look up embedding vectors for tokens\n",
        "        x = self.token_embedding(x) # [B,S,d_model]\n",
        "\n",
        "        # apply positional embedding\n",
        "        x = self.positional_embedding(x) # [B,S,d_model]\n",
        "\n",
        "        # apply sequence of masked self-attention blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x,mask=mask) # [B,S,d_model]\n",
        "\n",
        "        # produce sequence of output vectors\n",
        "        y = self.output(x) # [B,S,vocabulary_size]\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_RnrqhrY0rd"
      },
      "source": [
        "This function produces masks appropriate for sequence prediction.  The mask ensures that the output token at time t+1 only sees the generated sequence up to time t."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU1NLlLaY0re"
      },
      "outputs": [],
      "source": [
        "def make_mask(seq_len):\n",
        "    \"\"\" Make a mask for sequence prediction. \"\"\"\n",
        "    return (torch.triu(torch.ones((1,seq_len,seq_len)), diagonal=1)==1)\n",
        "\n",
        "make_mask(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MJDT8ZIY0rf"
      },
      "source": [
        "Now we will make a sequence of integers and see if the Transformer decoder can learn the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOt-gz-_Y0rf"
      },
      "outputs": [],
      "source": [
        "seq = torch.arange(100)\n",
        "x = seq[:-1][None,...]\n",
        "y = seq[1:][None,...]\n",
        "mask = make_mask(x.shape[1])\n",
        "x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Jgi27v3kY0rg"
      },
      "outputs": [],
      "source": [
        "steps = 1000\n",
        "# steps = 6\n",
        "model = TransformerDecoder(vocabulary_size=100,max_seq_len=x.shape[1],\n",
        "                           d_model=64,num_heads=8,d_ff=512,num_blocks=3\n",
        "                           )\n",
        "\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(),lr=.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for step in range(steps):\n",
        "    model.train()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    y_pred = model(x,mask)\n",
        "    loss = loss_fn(y_pred.view(-1,y_pred.shape[-1]),y.view(-1))\n",
        "    loss.backward()\n",
        "\n",
        "    opt.step()\n",
        "\n",
        "    print(step,loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGBgs5yyY0rg"
      },
      "source": [
        "If the Transformer has learned the sequence correctly, this output will read 1, 2, 3, ..., 97, 98, 99."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfG8XL6CY0rg"
      },
      "outputs": [],
      "source": [
        "torch.argmax(model(x),-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUjhwKOGY0rg"
      },
      "source": [
        "2. What size context does the Transformer need in order to learn the above sequence?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFtRjGlmY0rh"
      },
      "source": [
        "The minimum size context I was able to find was 6 steps. So the transformer needed to be trained on the first 6% of the sequence to learn it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9ZpijvOY0rh"
      },
      "source": [
        "3. Now design a pattern that requires a larger context and see if the Transformer can learn it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doCQv6yKY0rh"
      },
      "outputs": [],
      "source": [
        "seq = [i for i in range(0,20) for j in range(i)]\n",
        "seq = torch.tensor(seq)\n",
        "x = seq[:-1][None,...]\n",
        "y = seq[1:][None,...]\n",
        "mask = make_mask(x.shape[1])\n",
        "x,y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(model(x),-1)"
      ],
      "metadata": {
        "id": "tbu4YbcTMBpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even with 1000 iterations the transformer did not learn it."
      ],
      "metadata": {
        "id": "_gtm5QiVOb1O"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}