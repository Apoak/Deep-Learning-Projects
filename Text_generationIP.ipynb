{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apoak/Deep-Learning-Projects/blob/main/Text_generationIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nffzTlBu0zL_"
      },
      "source": [
        "### Lab 8.3 Text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hnyyd4Y0zMC"
      },
      "source": [
        "In this lab you will finish building your RNN text generator.  I found that this code actually runs pretty quickly on my MacBook without GPU acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPghbXTY0zMC"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "seq_len = 20\n",
        "hidden_size = 100\n",
        "batch_size = 32\n",
        "lr = 3e-4\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchmetrics"
      ],
      "metadata": {
        "id": "ZESTScM_06Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPXncfQb0zMD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j67aUZEh0zME"
      },
      "source": [
        "Here's the code to download and prepare the sonnet dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jloRPvMA0zMF",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!wget --no-clobber \"https://www.dropbox.com/scl/fi/7r68l64ijemidyb9lf80q/sonnets.txt?rlkey=udb47coatr2zbrk31hsfbr22y&dl=1\" -O sonnets.txt\n",
        "text = (open(\"sonnets.txt\").read())\n",
        "text = text.lower().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq2OisV80zMF"
      },
      "outputs": [],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-keNOyPj0zMG"
      },
      "source": [
        "Here's my solution for the `CharacterDataset` class.\n",
        "\n",
        "Note that it returns an entire sequence of tokens for the target (unlike what we did on Monday where it only output a single token for the target.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpxFRfaW0zMG"
      },
      "outputs": [],
      "source": [
        "class CharacterDataset(Dataset):\n",
        "  def __init__(self,text,seq_len=100,device='cpu'):\n",
        "    \"\"\"\n",
        "    Initialize a dataset using character tokenization.\n",
        "    Arguments:\n",
        "      text: a string containing the dataset\n",
        "      seq_len: sequence length provided by __getitem__\n",
        "      device: device for PyTorch tensors\n",
        "    \"\"\"\n",
        "    self.text = text\n",
        "    self.seq_len = seq_len\n",
        "    self.vocabulary = ''.join(sorted(list(set(text))))\n",
        "    self.index_to_char = {n:char for n, char in enumerate(self.vocabulary)}\n",
        "    self.char_to_index = {char:n for n, char in enumerate(self.vocabulary)}\n",
        "    self.device = device\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\" Return the length of sequences in the dataset. \"\"\"\n",
        "    return len(self.text)-self.seq_len-1\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    \"\"\" Return the input and target sequences starting at given index. \"\"\"\n",
        "\n",
        "    text = self.text[idx:idx+self.seq_len+1]\n",
        "    tokens = self.encode(text)\n",
        "\n",
        "    return torch.tensor(tokens[:-1],device=self.device),torch.tensor(tokens[1:],device=self.device)\n",
        "\n",
        "  def encode(self,text):\n",
        "    \"\"\" Encode a string to a list of integer tokens. \"\"\"\n",
        "    return list(map(self.char_to_index.get,text))\n",
        "\n",
        "  def decode(self,tokens):\n",
        "    \"\"\" Decode a list of token integers into a string. \"\"\"\n",
        "    return ''.join(list(map(self.index_to_char.get,tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_cM8kow0zMH"
      },
      "outputs": [],
      "source": [
        "ds = CharacterDataset(text,seq_len=seq_len,device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PAZVB6V80zMH"
      },
      "outputs": [],
      "source": [
        "ds.encode(text[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3ltk7BB0zMI"
      },
      "outputs": [],
      "source": [
        "print(ds.decode(ds.encode(text[:100])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYv7RQ6h0zMI"
      },
      "outputs": [],
      "source": [
        "x, y = ds[0]\n",
        "x.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ8vpNSm0zMJ"
      },
      "outputs": [],
      "source": [
        "dl = DataLoader(ds,shuffle=True,batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5RBR-sX0zMJ"
      },
      "source": [
        "Here's my solution for the recurrent neural network (RNN) implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOJCccbM0zMJ"
      },
      "outputs": [],
      "source": [
        "class CharacterRNN(nn.Module):\n",
        "  def __init__(self,vocabulary_size,hidden_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocabulary_size,hidden_size)\n",
        "    self.hidden_size = hidden_size\n",
        "    self.U = nn.Linear(hidden_size,hidden_size)\n",
        "    self.W = nn.Linear(hidden_size,hidden_size)\n",
        "    self.act = nn.SiLU()\n",
        "    self.V = nn.Linear(hidden_size,vocabulary_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.embedding(x)\n",
        "    B,N = x.shape[:2]\n",
        "    h = torch.zeros(B,self.hidden_size).to(x.device)\n",
        "    Ux = self.U(x)\n",
        "    y = []\n",
        "    for i in range(N):\n",
        "      Wh = self.W(h)\n",
        "      h = self.act(Ux[:,i] + Wh)\n",
        "      y.append(self.V(h))\n",
        "    return torch.stack(y,dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV3zBVUP0zMK"
      },
      "outputs": [],
      "source": [
        "model = CharacterRNN(len(ds.vocabulary),hidden_size).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfEHPhmw0zMK"
      },
      "outputs": [],
      "source": [
        "x_batch, y_batch = next(iter(dl))\n",
        "x_batch.shape, y_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgyTeUSQ0zMK"
      },
      "outputs": [],
      "source": [
        "model(x_batch).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sByIXsOM0zMK"
      },
      "source": [
        "Finally here is my code to train the model.\n",
        "\n",
        "Note that I needed to use `.view()` to reshape the model output and target, becuase the loss and metric functions want the data to have shape [B,C] not [B,N,C]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZacQhBy90zMK"
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "metric = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=len(ds.vocabulary))\n",
        "metric.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ddMk1EmK0zML"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  pbar = tqdm(total=len(dl))\n",
        "  for x_batch, y_batch in dl:\n",
        "    opt.zero_grad()\n",
        "\n",
        "    y_pred = model(x_batch)\n",
        "    loss = loss_fn(y_pred.view(-1,len(ds.vocabulary)),y_batch.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    opt.step()\n",
        "\n",
        "    pbar.update(1)\n",
        "  pbar.close()\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  metric.reset()\n",
        "  pbar = tqdm(total=len(dl))\n",
        "  for x_batch, y_batch in dl:\n",
        "    y_pred = model(x_batch)\n",
        "    metric(y_pred.view(-1,len(ds.vocabulary)),y_batch.view(-1))\n",
        "    pbar.update(1)\n",
        "  pbar.close()\n",
        "\n",
        "  acc = metric.compute().item()\n",
        "\n",
        "  print(f'epoch {epoch}: {acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFUkZCde0zMM"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF1BjsPx0zMM"
      },
      "source": [
        "1. Write a deterministic function to generate text given some starter text.  The function should iteratively add characters to the prompt using the trained model.  This version should be deterministic, in that in always takes the most likely next character according to the model.\n",
        "\n",
        "Test the function by prompting it with the first 10 characters in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0k5Q5K-0zMM"
      },
      "outputs": [],
      "source": [
        "def generate_text_deterministic(model,prompt,num_to_generate=1000):\n",
        "  ds = CharacterDataset(text,seq_len=seq_len,device=device)\n",
        "  print(\"len vocab: \", len(ds.vocabulary))\n",
        "  x = ds.encode(prompt)\n",
        "  x = torch.tensor(x,device=device)\n",
        "  x = x.unsqueeze(0)\n",
        "  print(x.shape)\n",
        "  print(\"prompt encoded :\", x)\n",
        "  # print(\"Output: \", model(x))\n",
        "  output = model(x)\n",
        "  # output = torch.softmax(output,dim=1)\n",
        "  # print(output[0])\n",
        "\n",
        "  response = [torch.argmax(tensor).item() for tensor in output[0]]\n",
        "  # response = torch.tensor(response[0])\n",
        "  # response = response.unsqueeze(0).unsqueeze(0)\n",
        "  print(response)\n",
        "\n",
        "  # print(ds.decode(response))\n",
        "\n",
        "  # NEED AN ARGMAX of output\n",
        "  combined = x\n",
        "  for num in range(num_to_generate-1):\n",
        "    response = torch.tensor(response[num])\n",
        "    response = response.unsqueeze(0).unsqueeze(0)\n",
        "    combined = torch.cat((combined, response), dim=1) # ADD A DIMENSION TO THE RESPONSE TESNOR\n",
        "    # print(combined)\n",
        "    output = model(combined)\n",
        "    # output = torch.softmax(output,dim=1)\n",
        "    response = [torch.argmax(tensor).item() for tensor in output[0]]\n",
        "    # print(response)\n",
        "    # print(ds.decode(response))\n",
        "  # print(response)\n",
        "  print(ds.decode(response))\n",
        "  # return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:10])\n",
        "generate_text_deterministic(model,text[:10], num_to_generate=1000)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Q-XZwFSv-F-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aanY2H90zMM"
      },
      "source": [
        "3. Write a stochastic version of the text generation function.  This one should use `torch.multinomial` to sample the next character.  Note that you will need to apply `torch.softmax` to convert the model output to probabilities.  (In my experience if you don't this you end up with a CUDA error and you end up needing to restart your kernel, so be careful!)\n",
        "\n",
        "Test the function by prompting it with the first 10 characters in the dataset, and run the generation multiple times to verify the stochastic behavior."
      ]
    },
    {
      "source": [
        "def generate_text_stochastic(model,prompt,num_to_generate=1000):\n",
        "  sentence = \"\"\n",
        "  ds = CharacterDataset(text,seq_len=seq_len,device=device)\n",
        "  x = ds.encode(prompt)\n",
        "  x = torch.tensor(x,device=device)\n",
        "  x = x.unsqueeze(0)\n",
        "  print(x.shape)\n",
        "  print(\"prompt encoded :\", x)\n",
        "  # print(\"Output: \", model(x))\n",
        "  output = model(x)\n",
        "  output = torch.softmax(output,dim=2) # Changed dim to 2 for proper softmax application\n",
        "\n",
        "  # Get probabilities for the first character prediction\n",
        "  probs = output[0, 0]\n",
        "  response = [torch.multinomial(probs, num_samples=1).item()] # Sample one character\n",
        "\n",
        "  for num in range(num_to_generate-1):\n",
        "    response_tensor = torch.tensor(response[-1], device=device).unsqueeze(0).unsqueeze(0) # Use only the last generated character\n",
        "    combined = torch.cat((x, response_tensor), dim=1)\n",
        "    output = model(combined)\n",
        "    output = torch.softmax(output,dim=2)\n",
        "\n",
        "    # Get probabilities for the next character prediction\n",
        "    probs = output[0, -1] # Probabilities for the last generated character\n",
        "    next_char = torch.multinomial(probs, num_samples=1).item() # Sample the next character\n",
        "    response.append(next_char) # Add the sampled character to the response\n",
        "\n",
        "    sentence = sentence + ds.decode([next_char])\n",
        "  return sentence"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "G40pdGbXXUxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text_stochastic(model,text[:10], num_to_generate=1000)"
      ],
      "metadata": {
        "id": "Jrz5UiQgXaI-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}